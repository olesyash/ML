# -*- coding: utf-8 -*-
"""dl2023_class.ex3.ipynb

Automatically generated by Colaboratory.

"""

import numpy as np
from keras.datasets import cifar10
from keras.utils import to_categorical
import matplotlib.pyplot as plt
from sklearn import metrics

# Load CIFAR10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Preprocess data
x_train = x_train.astype("float32") / 255.0
x_test = x_test.astype("float32") / 255.0

# Flatten the data
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# One-hot encode the labels
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)


# Define the network parameters
n_input = x_train.shape[1]
n_hidden = 64
n_output = 10

# Initialize the weights and biases
W1 = np.random.randn(n_input, n_hidden)
b1 = np.zeros(n_hidden)
W2 = np.random.randn(n_hidden, n_output)
b2 = np.zeros(n_output)


# Define the ReLU activation function
def relu(x):
    return np.maximum(0, x)


# Define the derivative of the ReLU function
def relu_derivative(x):
    return np.where(x > 0, 1, 0)


# Define the softmax function
def softmax(x):
    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return e_x / e_x.sum(axis=1, keepdims=True)


# Define the cross entropy loss function
def cross_entropy(y_pred, y_true):
    return -np.sum(y_true * np.log(y_pred + 1e-9)) / y_true.shape[0]


# Training loop
epochs = 100
learning_rate = 0.001
lambda_ = 0.001
losses = []
batch_size = 100  # Define the batch size

for i in range(epochs):
    # Shuffle the data
    permutation = np.random.permutation(x_train.shape[0])
    x_train_shuffled = x_train[permutation]
    y_train_shuffled = y_train[permutation]

    for j in range(0, x_train.shape[0], batch_size):
        # Get the mini-batch
        x_batch = x_train_shuffled[j : j + batch_size]
        y_batch = y_train_shuffled[j : j + batch_size]

        # Forward propagation
        z1 = np.dot(x_batch, W1) + b1
        a1 = relu(z1)
        z2 = np.dot(a1, W2) + b2
        y_pred = softmax(z2)

        # Compute the loss with L2 regularization
        loss = cross_entropy(y_pred, y_batch) + (lambda_ / 2) * (
            np.sum(W1**2) + np.sum(W2**2)
        )
        losses.append(loss)

        # Backward propagation
        delta_y = y_pred - y_batch
        delta_hidden = np.dot(delta_y, W2.T) * relu_derivative(a1)

        # Update the weights and biases
        W2 -= learning_rate * np.dot(a1.T, delta_y) + lambda_ * W2
        b2 -= learning_rate * np.sum(delta_y, axis=0)
        W1 -= learning_rate * np.dot(x_batch.T, delta_hidden) + lambda_ * W1
        b1 -= learning_rate * np.sum(delta_hidden, axis=0)

        # Print the loss every 100 epochs
        if j % 100 == 0:
            print(f"Epoch {i}, Loss: {loss}")

print(f"minimum loss: {min(losses)}")
# Plot the loss
plt.plot(losses)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

# Test
# Forward propagation
z1 = np.dot(x_test, W1) + b1
a1 = relu(z1)
z2 = np.dot(a1, W2) + b2
y_pred = softmax(z2)

y_pred = np.argmax(y_pred, axis=1)
y_test = np.argmax(y_test, axis=1)

confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
class_names = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=class_names)
accuracy = metrics.accuracy_score(y_test, y_pred)
print(f"Accurancy: {accuracy * 100} %")
cm_display.plot()
plt.show()
